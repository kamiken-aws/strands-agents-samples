{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands AgentsとOllamaモデルを使用したローカルエージェントの構築\n",
    "\n",
    "このノートブックでは、Strands AgentとOllamaを使用してパーソナルエージェントを作成する方法をデモンストレーションします。このエージェントは、ファイル操作、ウェブ検索、システムコマンドなど様々なローカルタスクを実行できます。\n",
    "\n",
    "## Ollamaとは？\n",
    "\n",
    "[Ollama](https://ollama.com/)は、マシン上で大規模言語モデル（LLM）をローカル実行できるオープンソースフレームワークです。これらのモデルと対話するためのシンプルなAPIを提供し、外部サービスにデータを送信したくないプライバシー重視のアプリケーションに最適です。\n",
    "\n",
    "Ollamaの主な利点：\n",
    "- **プライバシー**: 全ての処理がローカルマシン上で実行\n",
    "- **APIコストなし**: 好きなだけ無料で使用可能\n",
    "- **オフライン対応**: インターネット接続不要で動作\n",
    "- **カスタマイズ**: 特定の用途向けにファインチューニング可能\n",
    "\n",
    "\n",
    "## エージェント詳細\n",
    "\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|機能                |説明                                              |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|使用機能            |Ollamaモデル - ファイル操作エージェントの作成       |\n",
    "|エージェント構造    |シングルエージェントアーキテクチャ                  |\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントアーキテクチャ\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップとインストール\n",
    "\n",
    "このノートブックを実行する前に、以下の準備をしてください：\n",
    "\n",
    "1. Ollamaのインストール: [https://ollama.com/download](https://ollama.com/download)の指示に従う\n",
    "2. Ollamaサーバーの起動: `ollama serve`\n",
    "3. Ollamaでモデルのダウンロード: `ollama pull llama3.2:1b`\n",
    "\n",
    "詳細な指示は[ドキュメント](https://cuddly-sniffle-lrmk2y7.pages.github.io/0.1.x/user-guide/concepts/model-providers/ollama/)を参照してください。\n",
    "\n",
    "このノートブックでは、SageMaker Studioとの互換性のためにLinuxディストリビューション用のOllamaをダウンロードします。これはAWSリードワークショップでWorkshop Studio上でコード実行するためのものです。ローカルでこのコードを実行する場合は、現在の環境に合わせてOllamaのダウンロード手順を調整してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will work on linux computers\n",
    "!curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['ollama', 'serve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# Import strands components\n",
    "from strands import Agent, tool\n",
    "from strands.models.ollama import OllamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is running:\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    print(\"✅ Ollama is running. Available models:\")\n",
    "    for model in response.json().get(\"models\", []):\n",
    "        print(f\"- {model['name']}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Ollama is not running. Please start Ollama before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カスタムツールの定義\n",
    "\n",
    "ツールは、エージェントが環境と対話するための関数です。以下にパーソナルエージェントに役立ついくつかのツールを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Operation Tools\n",
    "@tool\n",
    "def file_read(file_path: str) -> str:\n",
    "    \"\"\"Read a file and return its content. Supports both text and PDF files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to read\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the file\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if it's a PDF file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            import PyPDF2\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text if text.strip() else \"Error: Could not extract text from PDF\"\n",
    "        else:\n",
    "            # Regular text file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File '{file_path}' not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def file_write(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write content to a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file\n",
    "        content (str): The content to write to the file\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(content)\n",
    "        return f\"File '{file_path}' written successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_directory(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List files and directories in the specified path.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to list\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string listing all files and directories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        items = os.listdir(directory_path)\n",
    "        files = []\n",
    "        directories = []\n",
    "\n",
    "        for item in items:\n",
    "            full_path = os.path.join(directory_path, item)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(f\"Folder: {item}/\")\n",
    "            else:\n",
    "                files.append(f\"File: {item}\")\n",
    "\n",
    "        result = f\"Contents of {os.path.abspath(directory_path)}:\\n\"\n",
    "        result += (\n",
    "            \"\\nDirectories:\\n\" + \"\\n\".join(sorted(directories))\n",
    "            if directories\n",
    "            else \"\\nNo directories found.\"\n",
    "        )\n",
    "        result += (\n",
    "            \"\\n\\nFiles:\\n\" + \"\\n\".join(sorted(files)) if files else \"\\nNo files found.\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error listing directory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama駆動エージェントの作成\n",
    "\n",
    "これでOllamaモデルと上記のツールを使用してエージェントを作成します。\n",
    "\n",
    "注意: `execute_commands`、`search_files`などのツールを追加することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive system prompt for our agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful personal assistant capable of performing local file actions and simple tasks for the user.\n",
    "\n",
    "Your key capabilities:\n",
    "1. Read, understand, and summarize files.\n",
    "2. Create and write to files.\n",
    "3. List directory contents and provide information on the files.\n",
    "4. Summarize text content\n",
    "\n",
    "When using tools:\n",
    "- Always verify file paths before operations\n",
    "- Be careful with system commands\n",
    "- Provide clear explanations of what you're doing\n",
    "- If a task cannot be completed, explain why and suggest alternatives\n",
    "\n",
    "Always be helpful, concise, and focus on addressing the user's needs efficiently.\n",
    "\"\"\"\n",
    "\n",
    "model_id = (\n",
    "    \"llama3.2:3b\"  # You can change this to any model you have pulled with Ollama.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollamaモデルの設定\n",
    "Ollamaサービスが http://localhost:11434 で動作しており、`model_id`が上記に表示されたOllamaモデルのリストに含まれていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaModel(\n",
    "    model_id=model_id,\n",
    "    host=\"http://localhost:11434\",\n",
    "    max_tokens=4096,  # Adjust based on your model's capabilities\n",
    "    temperature=0.7,  # Lower for more deterministic responses, higher for more creative\n",
    "    top_p=0.9,  # Nucleus sampling parameter\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "local_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    model=ollama_model,\n",
    "    tools=[file_read, file_write, list_directory],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エージェントのテスト\n",
    "\n",
    "いくつかの例タスクでエージェントをテストしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agent(\n",
    "    \"Read the file in the path `sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf` and summarize it in 5 bullet points.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: List files in the current directory\n",
    "response = local_agent(\"Show me the files in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Create a sample file\n",
    "response = local_agent(\n",
    "    \"Create a file called 'sample.txt' with the content 'This is a test file created by my Ollama agent.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: create a readme file after reading and understanding multiple files\n",
    "response = local_agent(\"Create a readme.md for the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "このノートブックで、StandsとOllamaを使用してローカルパーソナルエージェントを作成しました。このエージェントはファイル操作（読み書き、追記）とテキストの要約/分析ができます。\n",
    "\n",
    "これにより、OllamaでAIモデルをローカル実行し、strandsのツールシステムの柔軟性を組み合わせた強力さが実証されました。必要に応じてさらに多くのツールを追加したり、異なるOllamaモデルを使用できます。\n",
    "\n",
    "### 次のステップ（アイデア）\n",
    "\n",
    "- エージェントの能力に影響を与える異なるOllamaモデルを試す\n",
    "- ウェブ検索、メール送信、カレンダー統合などの複雑なツールを追加\n",
    "- エージェントに過去の対話を記憶するメモリを実装\n",
    "- エージェントとの対話のためのシンプルなUIを作成"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
